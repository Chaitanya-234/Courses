<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" dir="ltr" lang="en-US">
<head>
    <title>Retrieval Augmented Generation (RAG)</title>
    <style type="text/css" media="screen">
		@import url( ../shared/style.css );
	</style>
	<script src="../shared/scormfunctions.js" type="text/javascript"></script>
	<script src="../shared/contentfunctions.js" type="text/javascript"></script>
</head>
<body>
<h1>Retrieval Augmented Generation (RAG)</h1>

<h2>The Knowledge Gap</h2>
<p>
    LLMs are trained on public data up to a cut-off date. They do not know your private enterprise data, internal documentation, or real-time news.
    <strong>RAG</strong> is the pattern of injecting relevant private data into the LLM's context window at query time.
</p>

<h2>The RetrieveUserProxyAgent</h2>
<p>
    AutoGen provides a specialized agent called <code>RetrieveUserProxyAgent</code> designed specifically for RAG.
    It simplifies the pipeline of:
</p>
<ol>
    <li><strong>Ingestion:</strong> Loading documents (PDF, TXT, MD) from a directory.</li>
    <li><strong>Chunking:</strong> Splitting text into manageable pieces.</li>
    <li><strong>Embedding:</strong> Converting text to vectors using an embedding model (e.g., OpenAI ada-002, ChromaDB).</li>
    <li><strong>Retrieval:</strong> Finding the top-k chunks relevant to the user's query.</li>
</ol>

<h2>Configuration</h2>
<p>
    The <code>retrieve_config</code> dictionary controls this behavior:
</p>
<ul>
    <li><strong>task:</strong> "code", "qa", or "default". Helps the agent prompt the LLM appropriately.</li>
    <li><strong>docs_path:</strong> List of file paths or URLs to ingest.</li>
    <li><strong>chunk_token_size:</strong> Size of text chunks (e.g., 2000 tokens).</li>
    <li><strong>vector_db:</strong> The database to store embeddings (default is usually a local ChromaDB instance).</li>
</ul>

<h2>RAG + Agents Workflow</h2>
<p>
    In a RAG workflow, the `RetrieveUserProxyAgent` replaces the standard `UserProxyAgent`.
    When the user asks a question, this agent first queries its vector database.
    It then pastes the retrieved context into the prompt sent to the `AssistantAgent`.
    The Assistant uses this "ground truth" to answer, significantly reducing hallucinations.
</p>

<h2>Advanced RAG</h2>
<p>
    For complex scenarios, you might need:
</p>
<ul>
    <li><strong>Hybrid Search:</strong> Combining keyword search with vector search.</li>
    <li><strong>Re-ranking:</strong> Using a second model to score the retrieved documents before sending them to the LLM.</li>
    <li><strong>Multi-Vector Retrieval:</strong> Storing summaries separately from raw text.</li>
</ul>

<h2>Summary</h2>
<p>
    RAG turns an AutoGen agent from a generalist into a specialist on <em>your</em> data. It is the primary method for building enterprise "Knowledge Bots".
</p>

<script type="text/javascript">
</script>
</body>
</html>